{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ciência dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Carlos Simodo\n",
    "\n",
    "Nome: Luiz Fernando da Silva Borges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção: Serão permitidos grupos de três pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisarão fazer um questionário de avaliação de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Obtenção dos tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: ***[Preencha aqui o id da sua conta. Ex: @hoxie_reader ]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados de autenticação do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. Não modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas para construção da base de dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Lula'\n",
    "\n",
    "#Quantidade mínima de mensagens capturadas:\n",
    "n = 3000\n",
    "\n",
    "#Quantidade mínima de mensagens para a base de treinamento:\n",
    "t = 668\n",
    "\n",
    "#Filtro de língua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "#api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documentação do tweepy\n",
    "i = 1\n",
    "msgs_crus = []\n",
    "for msg_crus in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():    \n",
    "    msgs_crus.append(msg_crus.full_text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apaga retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apaga_rts(tweets_capturados):\n",
    "    lista = tweets_capturados\n",
    "    for e in lista:\n",
    "        if e.startswith('rt'):\n",
    "            indice = lista.index(e)\n",
    "            del lista[indice]\n",
    "        else:\n",
    "            pass\n",
    "    return lista\n",
    "\n",
    "msgs = apaga_rts(msgs_crus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n"
     ]
    }
   ],
   "source": [
    "#Embaralhando as mensagens para reduzir um possível viés temporal de obtenção para a classificação\n",
    "\n",
    "shuffle(msgs)\n",
    "\n",
    "print(len(msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = list(set(list(msgs)))\n",
    "\n",
    "len(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que a API capturou 2000 tweets, decidimos prosseguir com 1002 tweets no total. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_cortada = msgs[:1002]\n",
    "msgs = lista_cortada\n",
    "\n",
    "len(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos a quantidade de tweets igualmente para cada membro do grupo efetuar a classificação manual dos tweets em: positivo, negativo e irrelevante "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifica se o arquivso não existem para não substituir um conjunto pronto\n",
    "if not os.path.isfile('./mensagens_1.xlsx') and os.path.isfile('./mensagens_2.xlsx'):\n",
    "    \n",
    "    #pega a quantidade de metade das mensagens escolhidas (deve ser par)\n",
    "    metade_mensagens = int(len(msgs)/2)\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer1 = pd.ExcelWriter(\"mensagens_1A.xlsx\")\n",
    "    writer2 = pd.ExcelWriter(\"mensagens_2A.xlsx\")\n",
    "    \n",
    "    #divide o conjunto de mensagens totais em duas planilhas, pois trata-se de um projeto em dupla\n",
    "    dft1 = pd.DataFrame({\"Tweets\": pd.Series(msgs[:metade_mensagens])})\n",
    "    dft2 = pd.DataFrame({\"Tweets\": pd.Series(msgs[metade_mensagens:])})\n",
    "    \n",
    "    #converte os data frames para arquivos .xlsx\n",
    "    dft1.to_excel(excel_writer = writer1, index = False)\n",
    "    writer1.save()\n",
    "    \n",
    "    dft2.to_excel(excel_writer = writer2, index = False)\n",
    "    writer2.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antes da etapa abaixo, os tweets foram classificados manualmente usando a interface de classificação desenvolvida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reunimos os arquivos com os tweets e suas etiquetas de classificação em um único arquivo Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#junta os dois arquivos excel em um unico\n",
    "df_mensagens_etiquetadas_1 = pd.read_excel('tabela_etiquetada_1.xlsx') \n",
    "df_mensagens_etiquetadas_2 = pd.read_excel('tabela_etiquetada_2.xlsx') \n",
    "\n",
    "#a coluna foi nomeada manualmente de 'Tweets'\n",
    "valores1 = df_mensagens_etiquetadas_1[['Tweets','Etiquetas']]\n",
    "valores2 = df_mensagens_etiquetadas_2[['Tweets','Etiquetas']]\n",
    "\n",
    "df_tabela_etiquetada = pd.concat([valores1, valores2])\n",
    "#df_tabela_etiquetada\n",
    "#print(len(df_tabela_etiquetada['Etiquetas']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.422156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.745568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Etiquetas\n",
       "count  1002.000000\n",
       "mean      2.422156\n",
       "std       0.745568\n",
       "min       1.000000\n",
       "25%       2.000000\n",
       "50%       3.000000\n",
       "75%       3.000000\n",
       "max       3.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tabela_etiquetada.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analise dos tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos se existe pelo menos 252 tweets de cada tipo (positivo, negativo e irrelevante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#verifica se o numero de cada tipo etiquetas ultrapassa ou alcanca o minimo necessario\n",
    "distribuicao = df_tabela_etiquetada['Etiquetas'].value_counts().plot.bar()\n",
    "print(distribuicao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separacao dos tweets nos dataframes de treino e de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separa_teste_e_treino_igualitario(df):\n",
    "    #define a divisao\n",
    "    divisao=1/3\n",
    "    \n",
    "    #separa os dataframes em dois novos dataframes\n",
    "    df_teste, df_treino= np.split(df, [int(divisao*len(df))])\n",
    "\n",
    "    return df_teste, df_treino\n",
    "\n",
    "df_teste = separa_teste_e_treino_igualitario(df_tabela_etiquetada)[0]\n",
    "df_treino = separa_teste_e_treino_igualitario(df_tabela_etiquetada)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conta o numero de cada tipo de tweets no dataframe de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[167 103  64]\n",
      "[412 164  92]\n",
      "1002\n"
     ]
    }
   ],
   "source": [
    "#conta o numero de vezes que cada etiqueta se repete e coloca numa lista\n",
    "distribuicao = df_teste['Etiquetas'].value_counts()\n",
    "lista_dos_numeros_de_cada_tipo_de_etiqueta_teste = distribuicao.to_numpy()\n",
    "print(lista_dos_numeros_de_cada_tipo_de_etiqueta_teste)\n",
    "\n",
    "distribuicao = df_treino['Etiquetas'].value_counts()\n",
    "lista_dos_numeros_de_cada_tipo_de_etiqueta_treino = distribuicao.to_numpy()\n",
    "print(lista_dos_numeros_de_cada_tipo_de_etiqueta_treino)\n",
    "\n",
    "print(sum(lista_dos_numeros_de_cada_tipo_de_etiqueta_teste + lista_dos_numeros_de_cada_tipo_de_etiqueta_treino))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos filtrar os dois conjuntos de dados para\n",
    "    \n",
    "* Remover pontuações\n",
    "* Remover links\n",
    "* Tornar todas as letras minísculas \n",
    "* Colocar um espaço antes e um depois de cada emoji\n",
    "* Substituir mais de um espaçamento entre dois elementos por apenas um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "#Remove as pontuações !-.:?;,%&*_+-\\/ e links\n",
    "def limpa_pontuacao_links(tweet_1):\n",
    "    \n",
    "    pontuacoes = '[!-.:?;,%&*_+-\\/|]'\n",
    "    padrao = re.compile(pontuacoes)\n",
    "    tweet_sem_pontuacoes = re.sub(padrao, \"\", tweet_1)\n",
    "    \n",
    "    tweet_sem_links = re.sub(\"http[^\\s]*\", \"\", tweet_sem_pontuacoes)\n",
    "    \n",
    "    tweet_sem_links = tweet_sem_links.replace(\"\\n\", \" \") ################# COMO TIRAR COM REGEX??!!\n",
    "    \n",
    "    return tweet_sem_links\n",
    "\n",
    "#minha_string = minha_string.replace('\\n', '')\n",
    "\n",
    "#faz com que cada emoji tenha um espaço antes e depois dele\n",
    "def cerca_emoji(tweet_3):\n",
    "    lista_str_letras=\"\"\n",
    "    for letra in tweet_3:\n",
    "        if letra in emoji.UNICODE_EMOJI:\n",
    "            lista_str_letras = lista_str_letras + \" \" + letra + \" \"\n",
    "        else:\n",
    "            lista_str_letras = lista_str_letras + letra\n",
    "\n",
    "    return lista_str_letras\n",
    "\n",
    "#faz com que todas as letras sejam minúsculas e com apenas um espaço entre duas palavras\n",
    "def minusculo_espacos(tweet_3):\n",
    "    \n",
    "    tweet_minusculo = tweet_3.lower()\n",
    "        \n",
    "    tweet_espacos = re.sub(\" +\",\" \", tweet_minusculo)\n",
    "    \n",
    "    return tweet_espacos\n",
    "\n",
    "#aplica as três funções acima de uma vez\n",
    "def filtro(tweet):\n",
    "    tweet = limpa_pontuacao_links(tweet)\n",
    "    tweet = cerca_emoji(tweet)\n",
    "    tweet = minusculo_espacos(tweet)\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica os filtros citados para a coluna de tweets do dataframe de treino e salva em um novo dataframe filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>rt @uolnoticias pt confirma candidatura de ben...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>boa tarde patriotas deus é conservador o diabo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>@uol vamos seguir a cartilha do lula então sig...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>@nosdaimprensa @teteduche @xicosa leiase a est...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>@prsmalacheia @p0mb4g1ra burrismo mesmo foi qu...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>rt @ptnosenado perseguição repetida lava jato ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>@ferreiramaromba @raquela38128936 @lulaoficial...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweets  Etiquetas\n",
       "334  rt @uolnoticias pt confirma candidatura de ben...          3\n",
       "335  boa tarde patriotas deus é conservador o diabo...          2\n",
       "336  @uol vamos seguir a cartilha do lula então sig...          3\n",
       "337  @nosdaimprensa @teteduche @xicosa leiase a est...          3\n",
       "338  @prsmalacheia @p0mb4g1ra burrismo mesmo foi qu...          3\n",
       "339  rt @ptnosenado perseguição repetida lava jato ...          2\n",
       "340  @ferreiramaromba @raquela38128936 @lulaoficial...          1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_treino_filtrado = df_treino\n",
    "df_treino_filtrado[\"Tweets\"]  = df_treino_filtrado[\"Tweets\"].apply(filtro)\n",
    "df_treino_filtrado.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica os filtros citados para a coluna de tweets do dataframe de teste e salva em um novo dataframe filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt @emirsader lula não adianta eleger um presi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@macunaimaz @coelho7l @ath1abalba @eutrilheiro...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@veramagalhaes chama o lula</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amo que a luizianne vai se candidatar a prefei...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@belgranosouza @sidestranhe @rfmron @fernandoh...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@nane12958293 @lulemedeiros @lulaoficial vocês...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@ladyfontenelle são paulo já está quase sem na...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Etiquetas\n",
       "0  rt @emirsader lula não adianta eleger um presi...          3\n",
       "1  @macunaimaz @coelho7l @ath1abalba @eutrilheiro...          3\n",
       "2                        @veramagalhaes chama o lula          1\n",
       "3  amo que a luizianne vai se candidatar a prefei...          2\n",
       "4  @belgranosouza @sidestranhe @rfmron @fernandoh...          3\n",
       "5  @nane12958293 @lulemedeiros @lulaoficial vocês...          1\n",
       "6  @ladyfontenelle são paulo já está quase sem na...          2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_teste_filtrado = df_teste\n",
    "df_teste_filtrado[\"Tweets\"]  = df_teste_filtrado[\"Tweets\"].apply(filtro)\n",
    "df_teste_filtrado.head(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as series de tweets positivos, negativos e irrelevantes de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro_positivo_treino = df_treino_filtrado[\"Etiquetas\"] == 1\n",
    "filtro_negativo_treino =  df_treino_filtrado[\"Etiquetas\"] == 2\n",
    "filtro_irrelevante_treino =  df_treino_filtrado[\"Etiquetas\"] == 3\n",
    "\n",
    "positivo_treino = df_treino_filtrado.loc[filtro_positivo_treino]\n",
    "negativo_treino = df_treino_filtrado.loc[filtro_negativo_treino]\n",
    "irrelevante_treino = df_treino_filtrado.loc[filtro_irrelevante_treino]\n",
    "\n",
    "serie_positivo_treino = positivo_treino[\"Tweets\"]\n",
    "serie_negativo_treino = negativo_treino[\"Tweets\"]\n",
    "serie_irrelevante_treino = irrelevante_treino[\"Tweets\"]\n",
    "serie_todos_tweets_treino = pd.concat([serie_positivo_treino, serie_negativo_treino, serie_irrelevante_treino])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o classificador\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separando as séries de tweets positivos, negativos e irrelantes do conjunto de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_positivo_treino = \" \".join(serie_positivo_treino)\n",
    "serie_positivo_treino = pd.Series(string_positivo_treino.split())\n",
    "\n",
    "string_negativo_treino = \" \".join(serie_negativo_treino)\n",
    "serie_negativo_treino = pd.Series(string_negativo_treino.split())\n",
    "\n",
    "string_irrelevante_treino = \" \".join(serie_irrelevante_treino)\n",
    "serie_irrelevante_treino = pd.Series(string_irrelevante_treino.split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando as tabelas de frequências relativas dos tweets positivos, negativos e irrelevantes do conjunto de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabela_relativa_positivo_treino = serie_positivo_treino.value_counts()\n",
    "tabela_relativa_negativo_treino = serie_negativo_treino.value_counts()\n",
    "tabela_relativa_irrelevante_treino = serie_irrelevante_treino.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando o conjunto que vai nos servir como o conjunto de todos os tweets existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_todos_tweets_treino = string_positivo_treino + string_negativo_treino + string_irrelevante_treino\n",
    "\n",
    "serie_todos_tweets_treino = pd.Series(string_todos_tweets_treino.split())\n",
    "\n",
    "tabela_relativa_todos_tweets_treino = serie_todos_tweets_treino.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "probP = len(serie_positivo_treino)/len(serie_todos_tweets_treino)\n",
    "probN = len(serie_negativo_treino)/len(serie_todos_tweets_treino)\n",
    "probI = len(serie_irrelevante_treino)/len(serie_todos_tweets_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1323338525441329 0.25571131879543096 0.6120846313603323\n",
      "1.0001298026998962\n"
     ]
    }
   ],
   "source": [
    "print(probP, probN, probI)\n",
    "soma = probP+probN+probI\n",
    "print(soma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando termos P(Tweet|Positivo), P(Tweet|Negativo), P(Tweet|Irrelevante) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_tweet_positivo(tweet_teste_1):\n",
    "    prob_tweet_positivo = 1            #evita underlow\n",
    "    for palavra in tweet_teste_1.split():\n",
    "        prob_palavra_tweet_positivo = tabela_relativa_positivo_treino[palavra]\n",
    "        prob_tweet_positivo = prob_tweet_positivo * prob_palavra_tweet_positivo\n",
    "        \n",
    "    return prob_tweet_positivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_tweet_negativo(tweet_teste_2):\n",
    "    prob_tweet_negativo = 1             #evita underlow\n",
    "    for palavra in tweet_teste_2.split():\n",
    "        prob_palavra_tweet_negativo = tabela_relativa_negativo_treino[palavra]\n",
    "        prob_tweet_negativo = prob_tweet_negativo * prob_palavra_tweet_negativo\n",
    "        \n",
    "    return prob_tweet_negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_tweet_irrelevante(tweet_teste_3):\n",
    "    prob_tweet_irrelevante = 1             #evita underlow\n",
    "    for palavra in tweet_teste_3.split():\n",
    "        prob_palavra_tweet_irrelevante = tabela_relativa_irrelevante_treino[palavra]\n",
    "        prob_tweet_irrelevante = prob_tweet_irrelevante * prob_palavra_tweet_irrelevante\n",
    "        \n",
    "    return prob_tweet_irrelevante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando os termos P(Positivo|Tweet), P(Negativo|Tweet), P(Irrelavante|Tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def maior_prob(tweet_4):\n",
    "    prob_positivo_dado_tweet = probP * prob_tweet_positivo(tweet_4)\n",
    "    prob_negativo_dado_tweet = probN * prob_tweet_negativo(tweet_4)\n",
    "    prob_irrelevante_dado_tweet = probI * prob_tweet_irrelevante(tweet_4)\n",
    "    \n",
    "    if prob_positivo_dado_tweet > prob_negativo_dado_tweet and prob_positivo_dado_tweet > \\\n",
    "    prob_irrelevante_dado_tweet:\n",
    "        return 1\n",
    "    if prob_negativo_dado_tweet > prob_positivo_dado_tweet and  prob_negativo_dado_tweet > \\\n",
    "    prob_irrelevante_dado_tweet:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "        \n",
    "\n",
    "tweet = \"@uol\"\n",
    "\n",
    "print(maior_prob(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "Esta etapa é manual. Faça a mesma pelo Excel.\n",
    "\n",
    "**Importante: Caso classifique um percentual pequeno de tweets relevantes ou de não relevantes, deve voltar a este notebook e coletar mais tweets diferentes do produto escolhido pelo grupo.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
