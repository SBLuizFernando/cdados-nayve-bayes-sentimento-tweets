{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1 - Ci√™ncia dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Carlos Simodo\n",
    "\n",
    "Nome: Luiz Fernando da Silva Borges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aten√ß√£o: Ser√£o permitidos grupos de tr√™s pessoas, mas com uma rubrica mais exigente. Grupos deste tamanho precisar√£o fazer um question√°rio de avalia√ß√£o de trabalho em equipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Obten√ß√£o dos tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "#Instalando o tweepy\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta: ***[Preencha aqui o id da sua conta. Ex: @hoxie_reader ]***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dados de autentica√ß√£o do twitter:\n",
    "\n",
    "#Coloque aqui o identificador da conta no twitter: @fulano\n",
    "\n",
    "#leitura do arquivo no formato JSON\n",
    "with open('auth.pass') as fp:    \n",
    "    data = json.load(fp)\n",
    "\n",
    "#Configurando a biblioteca. N√£o modificar\n",
    "auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "auth.set_access_token(data['access_token'], data['access_token_secret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas para constru√ß√£o da base de dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de um produto e coleta das mensagens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Produto escolhido:\n",
    "produto = 'Lula'\n",
    "\n",
    "#Quantidade m√≠nima de mensagens capturadas:\n",
    "n = 3000\n",
    "\n",
    "#Quantidade m√≠nima de mensagens para a base de treinamento:\n",
    "t = 668\n",
    "\n",
    "#Filtro de l√≠ngua, escolha uma na tabela ISO 639-1.\n",
    "lang = 'pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturando os dados do twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um objeto para a captura\n",
    "#api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "#Inicia a captura, para mais detalhes: ver a documenta√ß√£o do tweepy\n",
    "i = 1\n",
    "msgs_crus = []\n",
    "for msg_crus in tweepy.Cursor(api.search, q=produto, lang=lang, tweet_mode=\"extended\").items():    \n",
    "    msgs_crus.append(msg_crus.full_text.lower())\n",
    "    i += 1\n",
    "    if i > n:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apaga retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apaga_rts(tweets_capturados):\n",
    "    lista = tweets_capturados\n",
    "    for e in lista:\n",
    "        if e.startswith('rt'):\n",
    "            indice = lista.index(e)\n",
    "            del lista[indice]\n",
    "        else:\n",
    "            pass\n",
    "    return lista\n",
    "\n",
    "msgs = apaga_rts(msgs_crus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n"
     ]
    }
   ],
   "source": [
    "#Embaralhando as mensagens para reduzir um poss√≠vel vi√©s temporal de obten√ß√£o para a classifica√ß√£o\n",
    "\n",
    "shuffle(msgs)\n",
    "\n",
    "print(len(msgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = list(set(list(msgs)))\n",
    "\n",
    "len(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que a API capturou 2000 tweets, decidimos prosseguir com 1002 tweets no total. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1002"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_cortada = msgs[:1002]\n",
    "msgs = lista_cortada\n",
    "\n",
    "len(msgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos a quantidade de tweets igualmente para cada membro do grupo efetuar a classifica√ß√£o manual dos tweets em: positivo, negativo e irrelevante "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verifica se o arquivso n√£o existem para n√£o substituir um conjunto pronto\n",
    "if not os.path.isfile('./mensagens_1.xlsx') and os.path.isfile('./mensagens_2.xlsx'):\n",
    "    \n",
    "    #pega a quantidade de metade das mensagens escolhidas (deve ser par)\n",
    "    metade_mensagens = int(len(msgs)/2)\n",
    "    \n",
    "    #Abre o arquivo para escrita\n",
    "    writer1 = pd.ExcelWriter(\"mensagens_1A.xlsx\")\n",
    "    writer2 = pd.ExcelWriter(\"mensagens_2A.xlsx\")\n",
    "    \n",
    "    #divide o conjunto de mensagens totais em duas planilhas, pois trata-se de um projeto em dupla\n",
    "    dft1 = pd.DataFrame({\"Tweets\": pd.Series(msgs[:metade_mensagens])})\n",
    "    dft2 = pd.DataFrame({\"Tweets\": pd.Series(msgs[metade_mensagens:])})\n",
    "    \n",
    "    #converte os data frames para arquivos .xlsx\n",
    "    dft1.to_excel(excel_writer = writer1, index = False)\n",
    "    writer1.save()\n",
    "    \n",
    "    dft2.to_excel(excel_writer = writer2, index = False)\n",
    "    writer2.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antes da etapa abaixo, os tweets foram classificados manualmente usando a interface de classifica√ß√£o desenvolvida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reunimos os arquivos com os tweets e suas etiquetas de classifica√ß√£o em um √∫nico arquivo Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#junta os dois arquivos excel em um unico\n",
    "df_mensagens_etiquetadas_1 = pd.read_excel('tabela_etiquetada_1.xlsx') \n",
    "df_mensagens_etiquetadas_2 = pd.read_excel('tabela_etiquetada_2.xlsx') \n",
    "\n",
    "#a coluna foi nomeada manualmente de 'Tweets'\n",
    "valores1 = df_mensagens_etiquetadas_1[['Tweets','Etiquetas']]\n",
    "valores2 = df_mensagens_etiquetadas_2[['Tweets','Etiquetas']]\n",
    "\n",
    "df_tabela_etiquetada = pd.concat([valores1, valores2])\n",
    "#df_tabela_etiquetada\n",
    "#print(len(df_tabela_etiquetada['Etiquetas']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1002.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.422156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.745568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Etiquetas\n",
       "count  1002.000000\n",
       "mean      2.422156\n",
       "std       0.745568\n",
       "min       1.000000\n",
       "25%       2.000000\n",
       "50%       3.000000\n",
       "75%       3.000000\n",
       "max       3.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tabela_etiquetada.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos se existe pelo menos 252 tweets de cada tipo (positivo, negativo e irrelevante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#verifica se o numero de cada tipo etiquetas ultrapassa ou alcanca o minimo necessario\n",
    "distribuicao = df_tabela_etiquetada['Etiquetas'].value_counts().plot.bar()\n",
    "print(distribuicao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[579 267 156]\n",
      "156\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#verifica se o numero de cada tipo etiquetas ultrapassa ou alcanca o minimo necessario\n",
    "\n",
    "    #valor minimo de cada tipo de etiqueta\n",
    "valor_minimo=252\n",
    "\n",
    "    #funcao responsavel pela verificacao\n",
    "def maior_que_250(df):\n",
    "    #define resposta padrao como verdadeiro\n",
    "    avancar=True\n",
    "    #conta o numero de vezes que cada etiqueta se repete e coloca numa lista\n",
    "    distribuicao = df['Etiquetas'].value_counts()\n",
    "    lista_dos_numeros_de_cada_tipo_de_etiqueta = distribuicao.to_numpy()\n",
    "    print(lista_dos_numeros_de_cada_tipo_de_etiqueta)\n",
    "    #caso algum dos numero nao passe, a resposta vai se tornar falsa\n",
    "    for valor in lista_dos_numeros_de_cada_tipo_de_etiqueta:\n",
    "        if valor>=valor_minimo:\n",
    "            pass\n",
    "        else:\n",
    "            print(valor)\n",
    "            avancar=False\n",
    "    return avancar\n",
    "\n",
    "print(maior_que_250(df_tabela_etiquetada))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cria uma lista para tipo de tweet (positivo, negativo e irrelevante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                                                Tweets  Etiquetas\n",
       " 379  fux virou a mesa: stf estremece ap√≥s decis√£o d...          1\n",
       " 193  @nesimachado perfeito! \\n\\nlula fala exatament...          1\n",
       " 250  droga o lula me fez concordar com ele https://...          1\n",
       " 56   rt @leonicemariana1: pt confirma candidatura d...          1\n",
       " 155  @ricardodpssouza @lulaoficial n√£o esque√ßa que ...          1\n",
       " ..                                                 ...        ...\n",
       " 181  @brendaprocon pra minha av√≥ vc vai ter que fal...          1\n",
       " 281  manipula√ß√£o de dela√ß√£o para acusar o advogado ...          1\n",
       " 329  @marilizpj pela primeira vez na vida, tenho qu...          1\n",
       " 206  manipula√ß√£o de dela√ß√£o para acusar o advogado ...          1\n",
       " 195  @danilogentili lula √© pessoa p√∫blica por sua n...          1\n",
       " \n",
       " [156 rows x 2 columns],\n",
       "                                                 Tweets  Etiquetas\n",
       " 152  @_sc_duda lula quer aparecer nem que seja pra ...          2\n",
       " 351           @frankcastle2009 verdade lula √© ladr√£ooo          2\n",
       " 121  @lulaoficial vamos esquecer vc lula.\\n\\num dia...          2\n",
       " 259  @fernandapsol e o calote que lula deu nos bene...          2\n",
       " 370  @leandroruschel argentina t√° parecendo o rio d...          2\n",
       " ..                                                 ...        ...\n",
       " 91   rt @assumpcad: s√≥cio do filho revela as falcat...          2\n",
       " 149  rt @tonystarkmeta: todos os antigos ‚Äúbrasileir...          2\n",
       " 88   rt @linatofolario: 4 milh√µes com recibo do ins...          2\n",
       " 239  @lulaoficial @dasilvabenedita @flaviodino @pcd...          2\n",
       " 174  @sf_moro √© t√£o merda que faz o lula ter raz√£o ...          2\n",
       " \n",
       " [156 rows x 2 columns],\n",
       "                                                 Tweets  Etiquetas\n",
       " 172  @datenaoficial datena, hoje voc√™ ficou com car...          3\n",
       " 9    @inst_lula @crisvector se de fato livrarem a c...          3\n",
       " 473  exclusivo: dcm entrevista lula https://t.co/jw...          3\n",
       " 249  rt @cynaramenezes: com o dem, com tudo: a alia...          3\n",
       " 218  @lelipefuckmann @fpshion7 @cleberbeachi @pco29...          3\n",
       " ..                                                 ...        ...\n",
       " 122  alerta! salles da xeque-mate no lula! treta en...          3\n",
       " 199  meu deus os coment√°rios üó£üó£üó£ jp perdeu a torcid...          3\n",
       " 131  acabo de descobrir q a caixa d'agua do campus ...          3\n",
       " 292  ex-presidente lula declara apoio a rubens http...          3\n",
       " 180  @sacha_faria sacha eu discordo! eu acredito qu...          3\n",
       " \n",
       " [156 rows x 2 columns]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mesmo_numero(df):\n",
    "    #distribuicao de etiquetas\n",
    "    distribuicao = df['Etiquetas'].value_counts()\n",
    "\n",
    "    #obtem a contagem de cada etiqueta\n",
    "    lista_dos_numeros_de_cada_tipo_de_etiqueta = distribuicao.to_numpy()\n",
    "\n",
    "    #descobre o numero de aparicoes da etiqueta menos frequente\n",
    "    menor_numero=min(lista_dos_numeros_de_cada_tipo_de_etiqueta)\n",
    "\n",
    "    #cria filtros\n",
    "    filtro_p = df['Etiquetas'] == 1\n",
    "    filtro_n = df['Etiquetas'] == 2\n",
    "    filtro_i = df['Etiquetas'] == 3\n",
    "\n",
    "    #filtra o dataframe\n",
    "    df_p = df.loc[filtro_p]\n",
    "    df_n = df.loc[filtro_n]\n",
    "    df_i = df.loc[filtro_i]\n",
    "    \n",
    "    #reorganiza os dataframes\n",
    "    df_p = df_p.sample(frac = 1) \n",
    "    df_n = df_n.sample(frac = 1) \n",
    "    df_i = df_i.sample(frac = 1) \n",
    "    \n",
    "    #slicing\n",
    "    lista_p = df_p.iloc[0:menor_numero,]\n",
    "    lista_n = df_n.iloc[0:menor_numero,]\n",
    "    lista_i = df_i.iloc[0:menor_numero,]\n",
    "    \n",
    "    lista_data_frames=[lista_p,lista_n,lista_i]\n",
    "    return lista_data_frames\n",
    "\n",
    "lista_data_frames = mesmo_numero(df_tabela_etiquetada)\n",
    "lista_data_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separa o conjunto de tweets positivos, negativos e irrelevantes em dois dataframes: treino e teste. O dataframe de treino possui 2/3 da quantidade total de tweets coletados. Dentro do dataframe de treino, a distribui√ß√£o entre tweets positivos, negativos e irrelevantes √© a mesma. O mesmo √© v√°lido para o 1/3 dos tweets separados no dataframe de teste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Tweets  Etiquetas\n",
      "379  fux virou a mesa: stf estremece ap√≥s decis√£o d...          1\n",
      "193  @nesimachado perfeito! \\n\\nlula fala exatament...          1\n",
      "250  droga o lula me fez concordar com ele https://...          1\n",
      "56   rt @leonicemariana1: pt confirma candidatura d...          1\n",
      "155  @ricardodpssouza @lulaoficial n√£o esque√ßa que ...          1\n",
      "..                                                 ...        ...\n",
      "163  rt @brasil247: com pantanal e amaz√¥nia em cham...          3\n",
      "72   @macunaimaz @profeborto @trotzer2 @ocyaradamas...          3\n",
      "263  caraio \\n\\no lula defendeu o bolsonaro kkkk ht...          3\n",
      "419  @sidestranhe @ursalinoal @rfmron @fernandoholi...          3\n",
      "313  @belgranosouza @puddan @fernandoholiday @kimpk...          3\n",
      "\n",
      "[156 rows x 2 columns]\n",
      "                                                Tweets  Etiquetas\n",
      "268  lula o melhor presidente do brasil. #moronacad...          1\n",
      "467                     @michellebelo7 lula o primeiro          1\n",
      "198  ‚Äòtemos de devolver ao presidente lula seus dir...          1\n",
      "14   @senadorhumberto velho isso j√° t√° de mais\\nou ...          1\n",
      "222  concordei com o lula, me julguem üòÇüòÇüòÇüòÇ https://...          1\n",
      "..                                                 ...        ...\n",
      "122  alerta! salles da xeque-mate no lula! treta en...          3\n",
      "199  meu deus os coment√°rios üó£üó£üó£ jp perdeu a torcid...          3\n",
      "131  acabo de descobrir q a caixa d'agua do campus ...          3\n",
      "292  ex-presidente lula declara apoio a rubens http...          3\n",
      "180  @sacha_faria sacha eu discordo! eu acredito qu...          3\n",
      "\n",
      "[312 rows x 2 columns]\n",
      "[52 52 52]\n",
      "[104 104 104]\n"
     ]
    }
   ],
   "source": [
    "#separar em teste e treino, tendo a mesma razao de porcentagem de cada tipo de etiqueta\n",
    "import numpy as np\n",
    "def separa_teste_treino(lista):\n",
    "    #define os termos da lista\n",
    "    df_p=lista[0]\n",
    "    df_n=lista[1]\n",
    "    df_i=lista[2]\n",
    "    \n",
    "    #define a divisao\n",
    "    divisao=1/3\n",
    "    \n",
    "    #separa os dataframes em dois novos dataframes\n",
    "    df_p_teste, df_p_treino= np.split(df_p, [int(divisao*len(df_p))])\n",
    "    df_n_teste, df_n_treino= np.split(df_n, [int(divisao*len(df_n))])\n",
    "    df_i_teste, df_i_treino= np.split(df_i, [int(divisao*len(df_i))])\n",
    "    \n",
    "    #junta os dfs parcias nos dfs de teste e treino\n",
    "    df_teste = pd.concat([df_p_teste, df_n_teste,df_i_teste],axis=0)\n",
    "    df_treino = pd.concat([df_p_treino, df_n_treino,df_i_treino],axis=0)\n",
    "    \n",
    "    #reorganiza os dfs\n",
    "    #df_teste = df_teste.sample(frac = 1)\n",
    "    #df_treino = df_treino.sample(frac = 1)\n",
    "\n",
    "    \n",
    "    #junta os dfs em uma lista\n",
    "    novos_dfs = [df_teste,df_treino]\n",
    "    return novos_dfs\n",
    "\n",
    "#chama a funcao\n",
    "novos_dfs=separa_teste_treino(lista_data_frames)\n",
    "df_teste = novos_dfs[0]\n",
    "df_treino = novos_dfs[1]\n",
    "print(df_teste)\n",
    "print(df_treino)\n",
    "\n",
    "#conta o numero de vezes que cada etiqueta se repete e coloca numa lista\n",
    "distribuicao = df_teste['Etiquetas'].value_counts()\n",
    "lista_dos_numeros_de_cada_tipo_de_etiqueta = distribuicao.to_numpy()\n",
    "print(lista_dos_numeros_de_cada_tipo_de_etiqueta)\n",
    "\n",
    "distribuicao = df_treino['Etiquetas'].value_counts()\n",
    "lista_dos_numeros_de_cada_tipo_de_etiqueta = distribuicao.to_numpy()\n",
    "print(lista_dos_numeros_de_cada_tipo_de_etiqueta)\n",
    "\n",
    "#testes\n",
    "#separa a funcao em df\n",
    "#df_p_teste = listas[0]\n",
    "#df_p_treino = listas[1]\n",
    "#df_n_teste = listas[2]\n",
    "#df_n_treino = listas[3]\n",
    "#df_i_teste = listas[4]\n",
    "#df_i_treino = listas[5]\n",
    "\n",
    "#print(len(df_p_teste.index))\n",
    "#print(len(df_p_treino.index))\n",
    "#print(len(df_n_teste.index))\n",
    "#print(len(df_n_treino.index))\n",
    "#print(len(df_i_teste.index))\n",
    "#print(len(df_i_treino.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos filtrar os dois conjuntos de dados para\n",
    "    \n",
    "* Remover pontua√ß√µes\n",
    "* Remover links\n",
    "* Tornar todas as letras min√≠sculas \n",
    "* Colocar um espa√ßo antes e um depois de cada emoji\n",
    "* Substituir mais de um espa√ßamento entre dois elementos por apenas um"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "#Remove as pontua√ß√µes !-.:?;,%&*_+-\\/ e links\n",
    "def limpa_pontuacao_links(tweet_1):\n",
    "    \n",
    "    pontuacoes = '[!-.:?;,%&*_+-\\/|]'\n",
    "    padrao = re.compile(pontuacoes)\n",
    "    tweet_sem_pontuacoes = re.sub(padrao, \"\", tweet_1)\n",
    "    \n",
    "    tweet_sem_links = re.sub(\"http[^\\s]*\", \"\", tweet_sem_pontuacoes)\n",
    "    \n",
    "    return tweet_sem_links\n",
    "\n",
    "#faz com que cada emoji tenha um espa√ßo antes e depois dele\n",
    "def cerca_emoji(tweet_3):\n",
    "    lista_str_letras=\"\"\n",
    "    for letra in tweet_3:\n",
    "        if letra in emoji.UNICODE_EMOJI:\n",
    "            lista_str_letras = lista_str_letras + \" \" + letra + \" \"\n",
    "        else:\n",
    "            lista_str_letras = lista_str_letras + letra\n",
    "\n",
    "    return lista_str_letras\n",
    "\n",
    "#faz com que todas as letras sejam min√∫sculas e com apenas um espa√ßo entre duas palavras\n",
    "def minusculo_espacos(tweet_3):\n",
    "    \n",
    "    tweet_minusculo = tweet_3.lower()\n",
    "    \n",
    "    tweet_espacos = re.sub(\" +\",\" \", tweet_minusculo)\n",
    "    \n",
    "    return tweet_espacos\n",
    "\n",
    "#aplica as tr√™s fun√ß√µes acima de uma vez\n",
    "def filtro(tweet):\n",
    "    tweet = limpa_pontuacao_links(tweet)\n",
    "    tweet = cerca_emoji(tweet)\n",
    "    tweet = minusculo_espacos(tweet)\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica os filtros citados para a coluna de tweets do dataframe de treino e salva em um novo dataframe filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>rt @comitelula ‚ö† Ô∏è urgente üö® \\n\\na defesa do e...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>@maluzikaaaa @nancycunhaa @jairbolsonaro ali√°s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>alerta salles da xequemate no lula treta entre...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>meu deus os coment√°rios üó£ üó£ üó£ jp perdeu a torc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>acabo de descobrir q a caixa dagua do campus d...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>expresidente lula declara apoio a rubens via @...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>@sachafaria sacha eu discordo eu acredito que ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweets  Etiquetas\n",
       "75   rt @comitelula ‚ö† Ô∏è urgente üö® \\n\\na defesa do e...          3\n",
       "247  @maluzikaaaa @nancycunhaa @jairbolsonaro ali√°s...          3\n",
       "122  alerta salles da xequemate no lula treta entre...          3\n",
       "199  meu deus os coment√°rios üó£ üó£ üó£ jp perdeu a torc...          3\n",
       "131  acabo de descobrir q a caixa dagua do campus d...          3\n",
       "292  expresidente lula declara apoio a rubens via @...          3\n",
       "180  @sachafaria sacha eu discordo eu acredito que ...          3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_treino_filtrado = df_treino\n",
    "df_treino_filtrado[\"Tweets\"]  = df_treino_filtrado[\"Tweets\"].apply(filtro)\n",
    "df_treino_filtrado.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplica os filtros citados para a coluna de tweets do dataframe de teste e salva em um novo dataframe filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Etiquetas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>o coringa √© um lula q nao deu certo\\nluan vitor</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>@marcelo41449088 @lulaoficial aff n√£o tinha me...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>rt @brasil247 com pantanal e amaz√¥nia em chama...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>@macunaimaz @profeborto @trotzer2 @ocyaradamas...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>caraio \\n\\no lula defendeu o bolsonaro kkkk</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>@sidestranhe @ursalinoal @rfmron @fernandoholi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>@belgranosouza @puddan @fernandoholiday @kimpk...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweets  Etiquetas\n",
       "267    o coringa √© um lula q nao deu certo\\nluan vitor          3\n",
       "454  @marcelo41449088 @lulaoficial aff n√£o tinha me...          3\n",
       "163  rt @brasil247 com pantanal e amaz√¥nia em chama...          3\n",
       "72   @macunaimaz @profeborto @trotzer2 @ocyaradamas...          3\n",
       "263       caraio \\n\\no lula defendeu o bolsonaro kkkk           3\n",
       "419  @sidestranhe @ursalinoal @rfmron @fernandoholi...          3\n",
       "313  @belgranosouza @puddan @fernandoholiday @kimpk...          3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_teste_filtrado = df_teste\n",
    "df_teste_filtrado[\"Tweets\"]  = df_teste_filtrado[\"Tweets\"].apply(filtro)\n",
    "df_teste_filtrado.tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando as series de tweets positivos, negativos e irrelevantes de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro_positivo_treino = df_treino_filtrado[\"Etiquetas\"] == 1\n",
    "filtro_negativo_treino =  df_treino_filtrado[\"Etiquetas\"] == 2\n",
    "filtro_irrelevante_treino =  df_treino_filtrado[\"Etiquetas\"] == 3\n",
    "\n",
    "positivo_treino = df_treino_filtrado.loc[filtro_positivo_treino]\n",
    "negativo_treino = df_treino_filtrado.loc[filtro_negativo_treino]\n",
    "irrelevante_treino = df_treino_filtrado.loc[filtro_irrelevante_treino]\n",
    "\n",
    "serie_positivo_treino = positivo_treino[\"Tweets\"]\n",
    "serie_negativo_treino = negativo_treino[\"Tweets\"]\n",
    "serie_irrelevante_treino = irrelevante_treino[\"Tweets\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando as series de tweets positivos, negativos e irrelevantes de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro_positivo_teste = df_teste_filtrado[\"Etiquetas\"] == 1\n",
    "filtro_negativo_teste =  df_teste_filtrado[\"Etiquetas\"] == 2\n",
    "filtro_irrelevante_teste =  df_teste_filtrado[\"Etiquetas\"] == 3\n",
    "\n",
    "positivo_teste = df_teste_filtrado.loc[filtro_positivo_teste]\n",
    "negativo_teste = df_teste_filtrado.loc[filtro_negativo_teste]\n",
    "irrelevante_teste = df_teste_filtrado.loc[filtro_irrelevante_teste]\n",
    "\n",
    "serie_positivo_teste = positivo_teste[\"Tweets\"]\n",
    "serie_negativo_teste = negativo_teste[\"Tweets\"]\n",
    "serie_irrelevante_teste = irrelevante_teste[\"Tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "Esta etapa √© manual. Fa√ßa a mesma pelo Excel.\n",
    "\n",
    "**Importante: Caso classifique um percentual pequeno de tweets relevantes ou de n√£o relevantes, deve voltar a este notebook e coletar mais tweets diferentes do produto escolhido pelo grupo.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
